{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/m20163692/anaconda3/envs/tf/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# Keras fix\n",
    "import tensorflow\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "tensorflow.python.control_flow_ops = control_flow_ops\n",
    "\n",
    "from keras.models import load_model\n",
    "from nl.text_gen import generate_from_model\n",
    "from nl.utils import load_w2v_data, GenerateSamples, print_word_list, parse_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n"
     ]
    }
   ],
   "source": [
    "print('Loading files...')\n",
    "\n",
    "text_file = 'text_files/asimov'\n",
    "model_file = 'models/asimov_model.h5'\n",
    "mode = 'general_1'\n",
    "\n",
    "# with tensorflow.device('/cpu:0'):\n",
    "#     model = load_model(model_file)\n",
    "\n",
    "model = load_model(model_file)\n",
    "\n",
    "seq_len = model.input_shape[1]\n",
    "input_dim = model.input_shape[2]\n",
    "\n",
    "word2idx, idx2word, embeddings = load_w2v_data('w2v_embs/asimov/asimov_vocab.json',\n",
    "                                               'w2v_embs/asimov/asimov_embeddings.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SAMPLE 1:\n",
      "— siempre me ha parecido entender que gladia no era feliz en\n",
      "\n",
      "TRUTH:\n",
      "— siempre me ha parecido entender que gladia no era feliz en su planeta natal, que abandonó su mundo con alegría y que en ningún momento tuvo intención de regresar. no obstante, estoy de acuerdo contigo en que la\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.250000):\n",
      "— siempre me ha parecido entender que gladia no era feliz en su mundo. \n",
      " — ¿qué es lo que no te parece? \n",
      " — ¿qué es lo que te hace, entonces? \n",
      " — ¿qué es lo que no te parece? \n",
      " — no lo sé. \n",
      " — ¿y si no le\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.500000):\n",
      "— siempre me ha parecido entender que gladia no era feliz en su vida. \n",
      " — ¿qué es lo que quieres? \n",
      " — si no es así, no lo sé. \n",
      " — ¿y qué tiene que ver con el problema? \n",
      " — en este momento, señor, no me lo he dicho. no\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.750000):\n",
      "— siempre me ha parecido entender que gladia no era feliz en el otro extremo. y no se podía decir tal cosa. ahora bien, ¿por qué había en todo esto el robot? \n",
      " — porque, de que no la hay, no un ser humano..., como no la más es un robot.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "SAMPLE 2:\n",
      "— sí, abuelo. piensa en ello. nunca había tenido\n",
      "\n",
      "TRUTH:\n",
      "— sí, abuelo. piensa en ello. nunca había tenido ocasión de hablar con stettin. la mayor parte del tiempo que has pasado con él estabais lejos del proyecto y yo pasaba casi todo mi tiempo encerrada en mi\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.250000):\n",
      "— sí, abuelo. piensa en ello. nunca había tenido que hacer nada. \n",
      " — ¿qué es lo que ha hecho, señor? \n",
      " — es una de las que no se han dado cuenta, ¿verdad? \n",
      " — no, no. no lo sé. \n",
      " — ¿y si lo que es\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.500000):\n",
      "— sí, abuelo. piensa en ello. nunca había tenido que ser más vida. \n",
      " — ¿y si yo le lo, señor? \n",
      " — porque es usted el que se ha oído decir, ¿verdad? \n",
      " — no. \n",
      " — ¿por qué no? \n",
      " — ¿qué es lo que es\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.750000):\n",
      "— sí, abuelo. piensa en ello. nunca había tenido que ver a te el mundo que yo no sea posible. \n",
      " — y la de la segunda fundación, ¿verdad? \n",
      " — sí, sí, pero lo que usted yo quiere hacer era que yo no sea los que han estado por esta parte del\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "SAMPLE 3:\n",
      "e hizo surgir un cubo hueco de color gris pizarra, mate\n",
      "\n",
      "TRUTH:\n",
      "e hizo surgir un cubo hueco de color gris pizarra, mate y monótono, en cuyo interior quedó encerrada el resto de la figura. a pesar de ello, la luz interior se veía brillar, aunque más apagada,\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.250000):\n",
      "e hizo surgir un cubo hueco de color gris pizarra, mate, y el que se había ido a la superficie de la tierra, y que no había oído hablar de él. \n",
      " — ¿y qué es lo que quiere? — preguntó trevize. \n",
      " — no, pero no lo sé. \n",
      " — ¿y\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.500000):\n",
      "e hizo surgir un cubo hueco de color gris pizarra, mate, y se la, por un momento, se la y el otro, con el rostro. \n",
      " — ¿y si no le importa? — preguntó el doctor fastolfe. \n",
      " — no, no. pero, ¿qué es lo que no me hace\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.750000):\n",
      "e hizo surgir un cubo hueco de color gris pizarra, mate, y el agua de un, aunque no tuvo más que un el día de la gente de la tierra, de lo cual ni siquiera yo me lo ¿no lo es? eso es algo que él me hace tan poco que son para usted y otros\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Random samples from text_file\n",
    "n_samples = 3\n",
    "n_words = 50\n",
    "print_truth = True\n",
    "n_truth_words = 30\n",
    "temperatures = [0.25, 0.5, 0.75]\n",
    "\n",
    "samples = GenerateSamples(word2idx, text_file, mode, 'dir', n_samples, n_words)\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "    print('-' * 80)\n",
    "    print('SAMPLE %d:' % (idx + 1))\n",
    "    print_word_list(sample[:seq_len])\n",
    "    \n",
    "    if print_truth:\n",
    "        print('\\nTRUTH:')\n",
    "        print_word_list(sample[:seq_len + n_truth_words])\n",
    "    \n",
    "    for temperature in temperatures:\n",
    "        print('\\nGENERATED FROM MODEL (temperature = %f):' % temperature)\n",
    "        model_sample = generate_from_model(sample, model, embeddings, idx2word,\n",
    "                                           word2idx, n_words, temperature, mode=mode)\n",
    "        print_word_list(model_sample)\n",
    "    \n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE TEXT:\n",
      "en el centro de nuestra galaxia comenzó\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.250000):\n",
      "en el centro de nuestra galaxia comenzó a estar de pie, y el otro, a un lado, se había dado cuenta de que el robot no era un robot, y el robot se la por la que se había ido, la más importante, el más pequeño de los mundos y el mismo los que han sido de la tierra, ¿verdad? \n",
      " — sí, señor. \n",
      " — ¿y si no es un robot? \n",
      " — no, no lo sé. \n",
      " — ¿y si no lo sea? \n",
      " — ¿por qué\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.500000):\n",
      "en el centro de nuestra galaxia comenzó la vez más a la vez. el doctor fastolfe se inclinó hacia atrás, y se fue de lo más. \n",
      " — ¿y qué es lo que nos? \n",
      " — no, no. no me — dijo, y añadió —: ¡no, no, no! en aquel momento... el que había estado en el otro extremo de la galaxia se hizo de nuevo la vida en la que se había dado cuenta, y después de todo, la respuesta de la palabra era la de un robot y un\n",
      "\n",
      "GENERATED FROM MODEL (temperature = 0.750000):\n",
      "en el centro de nuestra galaxia comenzó a estar de un lado a otro, había llegado a un punto, desde el de los con el doctor fastolfe le más de usted ella, ¿verdad? no lo tengo, ¿verdad? \n",
      " — no. no estoy seguro de que mi padre no tiene nada que ver con el asunto del doctor amadiro. \n",
      " — ¿por qué no? \n",
      " — si no lo me hace, si los la humanidad hubiese sido o por lo menos a los otros. \n",
      " ¡luego! y puede que no, así\n"
     ]
    }
   ],
   "source": [
    "# Using custom text\n",
    "text = 'en el centro de nuestra galaxia comenzó'\n",
    "n_words = 100\n",
    "temperatures = [0.25, 0.5, 0.75]\n",
    "\n",
    "print('SOURCE TEXT:\\n%s' % text)\n",
    "\n",
    "for temperature in temperatures:\n",
    "    print('\\nGENERATED FROM MODEL (temperature = %f):' % temperature)\n",
    "    model_sample = generate_from_model(text, model, embeddings, idx2word, word2idx,\n",
    "                                       n_words, temperature, truncating='post',\n",
    "                                       custom_text=True, mode=mode)\n",
    "    print_word_list(model_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tags_corpus = nltk.corpus.conll2002.tagged_words()\n",
    "tags_dict = {word:tag for word, tag in tags_corpus[:25000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lo\n",
      "le\n",
      "ellos\n",
      "la\n",
      "ellas\n",
      "nos\n",
      "nosotros\n",
      "los\n",
      "sí\n",
      "él\n",
      "les\n",
      "yo\n",
      "ello\n",
      "ella\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "www = []\n",
    "\n",
    "for word, tag in tags_corpus[:25000]:\n",
    "    word = word.lower()\n",
    "    if i == 25:\n",
    "        break\n",
    "        \n",
    "    if tag == 'PP':\n",
    "        try:\n",
    "            www.index(word)\n",
    "        except:\n",
    "            print(word)\n",
    "            www.append(word)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y si la verdad fuera de la tierra, ¿qué es lo que por. de que nosotros no sea el que le mundos, ¿verdad? \n",
      " — sí, señor. \n",
      " — ¿y si no es un robot? \n",
      " — no, no lo sé. \n",
      " — ¿del mismo modo que si todo el mundo puede que sea un robot, no puede ser que el doctor fastolfe sea un robot? \n",
      " — no, no lo sé. \n",
      " — ¿cómo te lo dijo? \n",
      " — porque sólo me he dado cuenta de que era un robot. \n",
      " — ¿por qué no? \n",
      " — no lo sé. \n",
      " — ¿por qué no? — dijo trevize —. si es necesario, eso no es lo que nos. \n",
      " — ¿y si el doctor amadiro es un robot? \n",
      " — es muy posible. \n",
      " — ¿y qué es eso? \n",
      " — no es. sólo hay un mundo que no tiene en cuenta la palabra, y no hay nada que la gente. — se interrumpió —. ¿es usted un robot? \n",
      " — después de todo, usted no es un robot. \n",
      " — ¿de qué manera? \n",
      " — es muy difícil de ser. le soy un robot. \n",
      " — ¿de qué manera? \n",
      " — es muy posible que el doctor fastolfe no sea la. \n",
      " — ¿por qué no lo aquí? \n",
      " — porque es un robot. \n",
      " — ¿de qué es lo que lo ha hecho? \n",
      " — de un modo tan ser..., todo lo que tenía que hacer.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nl.text_gen import get_next_word_distribution\n",
    "\n",
    "text = 'y si la verdad fuera'\n",
    "temperature = 0.65\n",
    "n_words = 300\n",
    "\n",
    "t = parse_text(text, word2idx, mode)\n",
    "tags = []\n",
    "tagged_words = []\n",
    "\n",
    "# def get_tag():\n",
    "    \n",
    "for i in range(n_words):\n",
    "    last_word = t[-1]\n",
    "    preds = get_next_word_distribution(t, model, embeddings, word2idx, temperature, mode=mode)\n",
    "    probs = np.random.multinomial(5, preds, 1).reshape(-1)\n",
    "    words = np.argsort(probs)[::-1]\n",
    "    test = 0\n",
    "    next_word = idx2word[words[test]]\n",
    "    \n",
    "    if last_word == next_word:\n",
    "        test += 1\n",
    "        next_word = words[test]\n",
    "    \n",
    "#     if 5-np.max(probs) < 2:\n",
    "#         print(next_word, '-', 5-np.max(probs))\n",
    "    \n",
    "    success = -1\n",
    "    tries = 0\n",
    "    \n",
    "    while(success != 0):\n",
    "        success = 0\n",
    "        \n",
    "        if tries > 0:\n",
    "            test += 1\n",
    "#             print('-try-')\n",
    "\n",
    "        next_word = idx2word[words[test]]\n",
    "        \n",
    "        try:\n",
    "            this_tag = tags_dict[next_word]\n",
    "        except:\n",
    "            this_tag = None\n",
    "\n",
    "        if this_tag:\n",
    "            try :\n",
    "                if tagged_words[-1] == tagged_words[-2] == next_word:\n",
    "                    success -= 1\n",
    "                    \n",
    "                if tags[-1] == this_tag == 'DA':\n",
    "                    success -= 1\n",
    "                    \n",
    "                if tags[-1] == this_tag == 'PP':\n",
    "                    success -= 1\n",
    "                    \n",
    "                if tags[-1] == this_tag == 'VSI':\n",
    "                    success -= 1\n",
    "            except:\n",
    "                None\n",
    "                \n",
    "        tries += 1\n",
    "       \n",
    "    tags.append(this_tag)\n",
    "    \n",
    "    if this_tag: \n",
    "        tagged_words.append(next_word)\n",
    "    \n",
    "    t.append(next_word)\n",
    "    \n",
    "print_word_list(t)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
